---
title: "Cohere tutorial: Text embedder with Cohere"
description: "We will compare random forest with Cohere Classifier to learn how to use Cohere for advanced text embedding and semantic search. Master the art of Cohere embedding today."
image: "https://imagedelivery.net/K11gkZF3xaVyYzFESMdWIQ/e3af1340-f1bb-4ed8-9fd9-c2dbf7076700/full"
authorUsername: AdiAdi
---
## Harnessing Cohere Playground for Exceptional Text Embedding and More

### Delving into the Core of Text Embedding

Text embedding in machine learning entails crafting a vector representation for a piece of text, which can then be fed into a machine learning algorithm. The crucial aspect of text embedding is to encapsulate the text's meaning in a format tailored for machine learning tasks.

### Decoding the Connection between Neural Networks and Text Embedding

Various techniques exist to create text embeddings, but neural networks are commonly utilized. As adept learning models, neural networks excel in discerning intricate relationships. These algorithms receive input vectors and output vectors of equivalent size, while mapping inputs to outputs in a way that captures the relationship.

### Generating Text Embedding through a Systematic Process

To achieve text embedding, a neural network initially undergoes training using an extensive corpus of text. This training data comprises sentences represented as vectors. The vectors are formed by summing word vectors from each sentence. Subsequently, the neural network is trained to map sentence vectors to a fixed vector size.

### Implementing Trained Neural Networks for Text Embedding

Once trained, neural networks effectively generate text embeddings for new textual inputs. The new text is vectorized, and the neural network maps the vector to the pre-determined size. The resulting text embedding accurately represents the text's meaning.

### Diverse Applications of Text Embeddings in Machine Learning

Text embeddings prove versatile within machine learning ventures, enhancing the performance of algorithms designed to classify texts. Furthermore, text embeddings can identify similar texts and group texts together.

### Determining the Optimal Text Embedding Technique

Multiple methods exist for text embedding, but the ideal one depends on the application. Nevertheless, neural networks are an influential and popular choice for generating high-quality text embeddings.

## Cohere

[Cohere](https://lablab.ai/tech/cohere) is a powerful neural network, which can generate, embed, and classify text. In this tutorial, we will use [Co:here](https://cohere.ai/) to embed descriptions. To use [Co:here](https://cohere.ai/) you need to create account on [Co:here](https://cohere.ai/) and get API key. 

<Img src="https://storage.googleapis.com/lablab-static-eu/images/tutorials/Chatbot-cohere-api.JPG" />

We will be programming in Python, so we need to install `cohere` library by `pip`
```python
pip install cohere
```

Firstly, we have to implement `cohere.Client`. In arguments of Client should be API key, which you have generated before, and version `2021-11-08`. I will create the class `CoHere`, it will be useful in the next steps.

```python
class CoHere:
    def __init__(self, api_key):
        self.co = cohere.Client(f'{api_key}', '2021-11-08')
        self.examples = []
```

## ðŸ’¾ Dataset

The main part of each neural network is a dataset. In this tutorial, I will use a dataset that includes 1000 descriptions of 10 classes. If you want to use the same, you can [download] (https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification?resource=download) it.

The downloaded dataset has 10 folders in each folder is 100 `files.txt` with descriptions. The name of files is a label of description, e.g.`sport_3.txt`.

We will compare `Random Forest` with `Co:here Classifier`, so we have to prepare data in two ways. For `Random Forest` we will use `Co:here Embedder`, we will focus on this in this tutorial. Cohere classifier requires samples, in which each sample should be designed as a list `[description, label]` and I did it in my previous [Cohere tutorial](https://lablab.ai/t/cohere-text-classifier)

### Loading paths of examples

In the beginning, we need to load all data, to do that. We create the function `load_examples`. In this function we will use three external libraries:

`os.path` to go into the folder with data. The code is executed in a path where is a python's `file.py`. This is an internal library, so we do not need to install it.

`numpy` this library is useful to work with arrays. In this tutorial, we will use it to generate random numbers. You have to install this library by pip `pip install numpy`. 

`glob` helps us to read all files and folder names. This is an external library, so the installation is needed - `pip install glob`.

The downloaded dataset should be extracted in the folder `data`. By `os.path.join` we can get universal paths of folders. 

```python
folders_path = os.path.join('data', '*')
```

In windows, a return is equal to `data\*`.

Then we can use `glob` method to get all names of folders.

```python
folders_name = glob(folders_path)
```
`folders_name` is a list, which contains window paths of folders. In this tutorial, these are the names of labels. 
```python
['data\\business', 'data\\entertainment', 'data\\food', 'data\\graphics', 'data\\historical', 'data\\medical', 'data\\politics', 'data\\space', 'data\\sport', 'data\\technologie']
```
Size of `Co:here` training dataset can not be bigger than 50 examples and each class has to have at least 5 examples, but for `Random Forest` we can use 1000 examples. With loop `for` we can get the names of each file. The entire function looks like that:

```python
import os.path
from glob import glob
import numpy as np

def load_examples(no_of_ex):
    examples_path = []

    folders_path = os.path.join('data', '*')
    folders_name = glob(folders_path)

    for folder in folders_name:
        files_path = os.path.join(folder, '*')
        files_name = glob(files_path)
        for i in range(no_of_ex // len(folders_name)):
            random_example = np.random.randint(0, len(files_name))
            examples_path.append(files_name[random_example])
    return examples_path
```

The last loop is taking randomly N-paths of each label and appending them into a new list `examples_path`.

### Load descriptions 

Now, we have to create a training set. To make it we will load examples with `load_examples()`. In each path is the name of a class, we will use it to create samples. Descriptions need to be read from files, a length can not be long, so in this tutorial, the length will be equal to 100. To list `texts` is appended list of `[descroption, class_name]`. Thus, a return is that list.

```python
def examples(no_of_ex):
    texts = []
    examples_path = load_examples(no_of_ex)
    for path in examples_path:
        class_name = path.split(os.sep)[1]
        with open(path, 'r', encoding="utf8") as file:
            text = file.read()[:100]
            texts.append([text, class_name])
    return texts
```

## ðŸ”¥ Co:here classifier

We back to `CoHere` class. We have to add one method - to embed examples.

The second `cohere` method is to embed text. The method has serval arguments, such as:

`model` size of a model.

`texts` list of texts to embed.

`truncate` if the text is longer than the available token, which part of the text should be taken `LEFT`, `RIGHT` or `NONE`.

All of them you can find [here](https://docs.cohere.ai/embed-reference).

In this tutorial, the `cohere` method will be implemented as a method of our `CoHere` class. 

```python
 def embed(self, no_of_ex):
        # as a good developer we should split the dataset. 
        data = pd.DataFrame(examples(no_of_ex))
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            list(data[0]), list(data[1]), test_size=0.2, random_state=0)
        # in the next two lines we create a numeric form of X_train data
        self.X_train_embeded = self.co.embed(texts=X_train,
                                              model="large",
                                              truncate="LEFT").embeddings
        self.X_test_embeded = self.co.embed(texts=X_test,
                                             model="large",
                                             truncate="LEFT").embeddings
```

`X_train_embeded` will be an array of numbers, which looks like that:
```
[ 386, 0.39653537, -0.409076, 0.5956299, -0.06624506, 2.0539167, 0.7133603,...
```

## ðŸ“ˆ Web application - Streamlit

To create an application, which will be comparison of two likelihood displays, we will use `Stramlit`. This is an easy and very useful library. 

Installation
```python
pip install streamlit
```

We will need text inputs for `co:here` API key. 

In docs of [streamlit](https://docs.streamlit.io/) we can find methods:

`st.header()` to make a header on our app

`st.test_input()` to send a text request

`st.button()` to create button

`st.write()` to display the results of cohere model.

`st.progress()` to display a progrss bar

`st.column()` to split an app


```python
st.header("Co:here Text Classifier vs Random Forest")

api_key = st.text_input("API Key:", type="password")

cohere = CoHere(api_key)

cohere.list_of_examples(50)  # number of examples for Cohere classifier
                             # showed in the previous tutorial 
cohere.embed(1000)           # number of examples for random forest

# initialization of random forest with sklearn library
forest = RandomForestClassifier(max_depth=10, random_state=0) 

col1, col2 = st.columns(2)

if col1.button("Classify"):
    # training process of random forest, to do it we use embedded text.
    forest.fit(cohere.X_train_embeded, cohere.y_train)
    # prediction process of random forest
    predict = forest.predict_proba(np.array(cohere.X_test_embeded[0]).reshape(1, -1))[0] 
    here = cohere.classify([cohere.X_test[0]])[0] # prediction process of cohere classifier
    col2.success(f"Correct prediction: {cohere.y_test[0]}") # display original label

    col1, col2 = st.columns(2)
    col1.header("Co:here classify") # predictions for cohere
    for con in here.confidence:
        col1.write(f"{con.label}: {np.round(con.confidence*100, 2)}%")
        col1.progress(con.confidence)

    col2.header("Random Forest") # predictions for random forest
    for con, pred in zip(here.confidence, predict):
        col2.write(f"{con.label}: {np.round(pred*100, 2)}%")
        col2.progress(pred)
```

To run the streamlit app use command
```python
streamlit run name_of_your_file.py
```

The created app looks like this

<Img src="https://storage.googleapis.com/lablab-static-eu/images/tutorials/cohere-text-embedder/apka.JPG" />


### ðŸ’¡ In Conclusion
Elevate your machine learning prowess with text embedding, delving into the realm of neural networks â€“ a widespread and effective approach for generating text embeddings. Apply these embeddings across various tasks like text categorization, similarity detection, and data clustering.

Throughout this guide, we have compared 'Random Forests' using 'Cohere Classifier,' showcasing the extensive potential of the 'Cohere Embedder.' This versatile toolkit can facilitate the development of numerous applications.

You can always test what you have learned during our upcoming [AI Hackathons](https://lablab.ai/event).

Stay tuned for more informative tutorials!

The repository of this code can check [here](https://github.com/AdBanacho/cohere-text-embedder).

**Thank you!** - [Adrian Banachowicz](https://www.linkedin.com/in/adrian-banachowicz/), Data Science Intern in [New Native](https://newnative.ai/)
